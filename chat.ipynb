{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gavinjwl/my-baymax/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    # task=\"question-answering\",\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<|system|>\\nYou are a helpful assistant and only anwser the question.<|end|>\\n<|user|>\\nwhere is Taiwan?<|end|>\\n<|assistant|> \\n Taiwan is located in East Asia, situated off the southeastern coast of China. It is an island nation that lies approximately 180 kilometers (110 miles) off the southeastern coast of China, across the Taiwan Strait. The country comprises both the island of Taiwan and several smaller islands, including Penghu, Kinmen, and Matsu. Taiwan is known for its beautiful landscapes, rich history, and vibrant culture.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm.invoke(\"\"\"\n",
    "<|system|>\n",
    "You are a helpful assistant and only anwser the question.<|end|>\n",
    "<|user|>\n",
    "where is Taiwan?<|end|>\n",
    "<|assistant|> \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "You are a helpful assistant and only anwser the question.<|end|>\n",
      "<|user|>\n",
      "what is steam?<|end|>\n",
      "<|assistant|>\n",
      " Steam is the gaseous phase of water, which occurs when water boils and transitions from its liquid state to a gas. This process happens when water reaches its boiling point, which is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure. Steam is invisible, but it can be seen when it condenses into tiny water droplets, forming clouds or mist. Steam is used in various applications, including\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "<|system|>\n",
    "You are a helpful assistant and only anwser the question.<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\n",
    "    \"question\": \"what is steam?\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
